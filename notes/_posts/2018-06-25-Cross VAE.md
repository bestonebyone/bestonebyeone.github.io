---
layout: post
category: notes
title: Tensorflow batchnorm
---

## One mistake of batchnorm

Problem: When the program is in test phase, I set the training = False, and then I just get result nan.

After I google it, I found I misuse the batch norm function. In the website of tensorflow, I found the standard usage of batch norm. It is shown below.

**Note: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op.**  
**One can set updates_collections=None to force the updates in place, but that can have a speed penalty, especially in distributed settings.**


<head>
    <title>Rouge</title>
    <link media="all" rel="stylesheet" href="/css/rouge.css" />
</head>

<body>
    {% highlight ruby %}
	update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
    with tf.control_dependencies(update_ops):
        train_op = optimizer.minimize(loss)
    {% endhighlight %}
</body>

Also, I found that batch norm may appear some mistake because of the batch size.

这里又出现了一个问题，我是两个网络公用shared latent space，其中一个网络需要batchnorm 另一个不需要，我直接在loss处加这段代码貌似出现了问题，更新的不太对。而我将它只加载batchnorm需要的那个网络那里，则看起来很正常。batchnorm的问题感觉很多，网上出现各种各样的问题。如果还有什么问题可以参考stack overflow的问题How could I use Batch Normalization in TensorFlow?